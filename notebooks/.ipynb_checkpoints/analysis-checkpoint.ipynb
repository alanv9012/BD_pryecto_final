{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Urban Mobility Pattern Analysis\n",
        "\n",
        "## Project: Analysis of Urban Mobility Patterns using Apache Spark\n",
        "\n",
        "This notebook implements a complete data analysis pipeline for understanding urban mobility patterns from taxi trip data.\n",
        "\n",
        "### Objectives:\n",
        "- Apply Apache Spark for processing and analyzing urban mobility data\n",
        "- Identify mobility patterns (peak hours, busiest zones, average trip duration, etc.)\n",
        "- Develop a complete data analysis workflow: ingestion, cleaning, transformation, analysis, and visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# Add src directory to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Import project modules\n",
        "from src.config import *\n",
        "from src.data_processing import *\n",
        "from src.zone_mapping import *\n",
        "from src.analysis import *\n",
        "from src.mongodb_operations import *\n",
        "\n",
        "print(\"Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MongoDB Configuration\n",
        "# TODO: Replace with your MongoDB Atlas connection string\n",
        "# Format: mongodb+srv://username:password@cluster.mongodb.net/\n",
        "MONGODB_URI = os.getenv(\"MONGODB_URI\", \"mongodb+srv://a271455_db_user:I3Sxtk54C3J5moXw@bigdatacluster.zagwkmh.mongodb.net/?appName=BigDataCluster\")\n",
        "\n",
        "# Update config if needed\n",
        "if MONGODB_URI != \"mongodb+srv://a271455_db_user:I3Sxtk54C3J5moXw@bigdatacluster.zagwkmh.mongodb.net/?appName=BigDataCluster\":\n",
        "    from src import config\n",
        "    config.MONGODB_URI = MONGODB_URI\n",
        "    print(\"MongoDB URI configured\")\n",
        "else:\n",
        "    print(\"WARNING: Please update MongoDB URI before saving to MongoDB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Spark session\n",
        "spark = create_spark_session()\n",
        "\n",
        "# Display Spark version and configuration\n",
        "print(f\"Spark Version: {spark.version}\")\n",
        "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
        "print(\"Spark session initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Ingestion and Initial Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load taxi data\n",
        "taxi_data_path = str(TAXI_DATA_DIR)\n",
        "print(f\"Loading data from: {taxi_data_path}\")\n",
        "\n",
        "df_raw = load_taxi_data(spark, taxi_data_path)\n",
        "\n",
        "# Display schema\n",
        "print(\"\\n=== Data Schema ===\")\n",
        "df_raw.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic statistics\n",
        "print(f\"Total records: {df_raw.count():,}\")\n",
        "print(f\"Total columns: {len(df_raw.columns)}\")\n",
        "print(f\"\\nColumn names: {df_raw.columns}\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\n=== Sample Data (first 5 rows) ===\")\n",
        "df_raw.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for null values in key columns\n",
        "from pyspark.sql.functions import col, isnan, isnull, when, count\n",
        "\n",
        "print(\"=== Null Value Analysis ===\")\n",
        "null_counts = df_raw.select([count(when(isnull(c) | isnan(c), c)).alias(c) for c in df_raw.columns])\n",
        "null_counts.show(vertical=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Cleaning and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize column names and clean data\n",
        "print(\"Starting data preprocessing...\")\n",
        "\n",
        "df_cleaned = preprocess_taxi_data(spark, taxi_data_path)\n",
        "\n",
        "print(f\"\\nPreprocessing complete!\")\n",
        "print(f\"Final record count: {df_cleaned.count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display cleaned data schema\n",
        "print(\"=== Cleaned Data Schema ===\")\n",
        "df_cleaned.printSchema()\n",
        "\n",
        "# Show sample of cleaned data\n",
        "print(\"\\n=== Sample Cleaned Data ===\")\n",
        "df_cleaned.select(\n",
        "    \"pickup_datetime\", \"dropoff_datetime\", \"trip_duration_min\",\n",
        "    \"hour_of_day\", \"day_of_week\", \"day_name\",\n",
        "    \"trip_distance_km\", \"fare_amount\", \"passenger_count\"\n",
        ").show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Enrichment - Zone Assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load zones dataset\n",
        "zones_path = str(ZONES_DATA_PATH)\n",
        "print(f\"Loading zones from: {zones_path}\")\n",
        "\n",
        "try:\n",
        "    zones_df = load_zones(spark, zones_path)\n",
        "    print(f\"Loaded {zones_df.count()} zones\")\n",
        "    zones_df.show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading zones file: {e}\")\n",
        "    print(\"Creating zones from data...\")\n",
        "    zones_df = create_zones_from_data(df_cleaned, num_zones=20)\n",
        "    zones_df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enrich trips with zone information\n",
        "print(\"Enriching trips with zone information...\")\n",
        "\n",
        "df_enriched = enrich_with_zones(df_cleaned, zones_df)\n",
        "\n",
        "print(\"Zone enrichment complete!\")\n",
        "print(f\"Records with pickup zone: {df_enriched.filter(col('pickup_zone_name').isNotNull()).count():,}\")\n",
        "print(f\"Records with dropoff zone: {df_enriched.filter(col('dropoff_zone_name').isNotNull()).count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample of enriched data\n",
        "print(\"=== Sample Enriched Data ===\")\n",
        "df_enriched.select(\n",
        "    \"pickup_datetime\", \"hour_of_day\", \"day_name\",\n",
        "    \"pickup_zone_name\", \"dropoff_zone_name\",\n",
        "    \"trip_duration_min\", \"trip_distance_km\", \"fare_amount\"\n",
        ").show(10, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exploratory Data Analysis\n",
        "\n",
        "### 6.1 Demand by Hour of Day\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze demand by hour\n",
        "hourly_demand = analyze_demand_by_hour(df_enriched)\n",
        "\n",
        "if hourly_demand:\n",
        "    print(\"=== Hourly Demand Analysis ===\")\n",
        "    hourly_demand.show(24, truncate=False)\n",
        "    \n",
        "    # Find peak hours\n",
        "    from pyspark.sql.functions import desc\n",
        "    peak_hour = hourly_demand.orderBy(desc(\"total_trips\")).first()\n",
        "    print(f\"\\nPeak hour: {peak_hour['hour_of_day']}:00 with {peak_hour['total_trips']:,} trips\")\n",
        "else:\n",
        "    print(\"Hourly analysis not available (missing hour_of_day column)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Demand by Day of Week\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze demand by day of week\n",
        "daily_demand = analyze_demand_by_day(df_enriched)\n",
        "\n",
        "if daily_demand:\n",
        "    print(\"=== Daily Demand Analysis ===\")\n",
        "    daily_demand.show(truncate=False)\n",
        "    \n",
        "    # Compare weekdays vs weekends\n",
        "    from pyspark.sql.functions import sum as spark_sum\n",
        "    weekday_weekend = daily_demand.groupBy(\"is_weekend\").agg(\n",
        "        spark_sum(\"total_trips\").alias(\"total_trips\"),\n",
        "        spark_sum(\"total_revenue\").alias(\"total_revenue\")\n",
        "    )\n",
        "    print(\"\\n=== Weekday vs Weekend Comparison ===\")\n",
        "    weekday_weekend.show(truncate=False)\n",
        "else:\n",
        "    print(\"Daily analysis not available (missing day_of_week column)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Zone Activity Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze zone activity\n",
        "zone_results = analyze_zone_activity(df_enriched)\n",
        "\n",
        "if zone_results.get(\"top_origin_zones\"):\n",
        "    print(\"=== Top 10 Origin Zones ===\")\n",
        "    zone_results[\"top_origin_zones\"].show(truncate=False)\n",
        "\n",
        "if zone_results.get(\"top_destination_zones\"):\n",
        "    print(\"\\n=== Top 10 Destination Zones ===\")\n",
        "    zone_results[\"top_destination_zones\"].show(truncate=False)\n",
        "\n",
        "if zone_results.get(\"combined_zone_activity\"):\n",
        "    print(\"\\n=== Top 20 Zones by Total Activity ===\")\n",
        "    zone_results[\"combined_zone_activity\"].show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Trip Duration and Distance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze trip duration and distance\n",
        "duration_distance_stats = analyze_trip_duration_distance(df_enriched)\n",
        "\n",
        "for stat_type, stats_df in duration_distance_stats:\n",
        "    print(f\"\\n=== Duration and Distance Statistics ({stat_type}) ===\")\n",
        "    stats_df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Revenue and Payment Type Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze revenue by payment type\n",
        "revenue_analysis = analyze_revenue_payment(df_enriched)\n",
        "\n",
        "if revenue_analysis:\n",
        "    print(\"=== Revenue Analysis by Payment Type ===\")\n",
        "    revenue_analysis.show(truncate=False)\n",
        "    \n",
        "    # Calculate total revenue\n",
        "    from pyspark.sql.functions import sum as spark_sum\n",
        "    total_revenue = revenue_analysis.agg(spark_sum(\"total_revenue\").alias(\"total_revenue\")).collect()[0][\"total_revenue\"]\n",
        "    print(f\"\\nTotal Revenue: ${total_revenue:,.2f}\")\n",
        "else:\n",
        "    print(\"Revenue analysis not available (missing payment_type column)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Storage - MongoDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare all analysis results for MongoDB\n",
        "analysis_results = {\n",
        "    \"trips_by_hour\": hourly_demand,\n",
        "    \"trips_by_day\": daily_demand,\n",
        "    \"zones_activity\": zone_results.get(\"combined_zone_activity\"),\n",
        "    \"revenue_analysis\": revenue_analysis\n",
        "}\n",
        "\n",
        "# Save to MongoDB (only if URI is configured)\n",
        "if MONGODB_URI and MONGODB_URI != \"mongodb+srv://username:password@cluster.mongodb.net/\":\n",
        "    try:\n",
        "        print(\"Saving analysis results to MongoDB...\")\n",
        "        save_analysis_results(\n",
        "            analysis_results,\n",
        "            MONGODB_URI,\n",
        "            MONGODB_DATABASE,\n",
        "            MONGODB_COLLECTIONS\n",
        "        )\n",
        "        print(\"\\nSuccessfully saved all results to MongoDB!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError saving to MongoDB: {e}\")\n",
        "        print(\"Please check your MongoDB connection string and network access.\")\n",
        "else:\n",
        "    print(\"MongoDB URI not configured. Skipping MongoDB storage.\")\n",
        "    print(\"Please update MONGODB_URI in the configuration cell to save to MongoDB.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Data for Power BI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure output directory exists\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Export all analysis results to CSV for Power BI\n",
        "print(\"Exporting data for Power BI...\")\n",
        "\n",
        "if hourly_demand:\n",
        "    export_for_powerbi(hourly_demand, str(OUTPUT_DIR / \"hourly_demand.csv\"))\n",
        "    print(\"✓ Exported hourly_demand.csv\")\n",
        "\n",
        "if daily_demand:\n",
        "    export_for_powerbi(daily_demand, str(OUTPUT_DIR / \"daily_demand.csv\"))\n",
        "    print(\"✓ Exported daily_demand.csv\")\n",
        "\n",
        "if zone_results.get(\"combined_zone_activity\"):\n",
        "    export_for_powerbi(\n",
        "        zone_results[\"combined_zone_activity\"],\n",
        "        str(OUTPUT_DIR / \"zone_activity.csv\")\n",
        "    )\n",
        "    print(\"✓ Exported zone_activity.csv\")\n",
        "\n",
        "if revenue_analysis:\n",
        "    export_for_powerbi(revenue_analysis, str(OUTPUT_DIR / \"revenue_analysis.csv\"))\n",
        "    print(\"✓ Exported revenue_analysis.csv\")\n",
        "\n",
        "print(\"\\nAll exports complete! Files are ready for Power BI import.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Key Findings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary statistics\n",
        "print(\"=== PROJECT SUMMARY ===\\n\")\n",
        "\n",
        "print(f\"Total trips analyzed: {df_enriched.count():,}\")\n",
        "\n",
        "if hourly_demand:\n",
        "    from pyspark.sql.functions import desc\n",
        "    peak = hourly_demand.orderBy(desc(\"total_trips\")).first()\n",
        "    print(f\"Peak hour: {peak['hour_of_day']}:00 ({peak['total_trips']:,} trips)\")\n",
        "\n",
        "if daily_demand:\n",
        "    busiest_day = daily_demand.orderBy(desc(\"total_trips\")).first()\n",
        "    print(f\"Busiest day: {busiest_day['day_name']} ({busiest_day['total_trips']:,} trips)\")\n",
        "\n",
        "if zone_results.get(\"combined_zone_activity\"):\n",
        "    top_zone = zone_results[\"combined_zone_activity\"].first()\n",
        "    print(f\"Most active zone: {top_zone['zone_name']} ({top_zone['total_activity']:,} trips)\")\n",
        "\n",
        "if revenue_analysis:\n",
        "    from pyspark.sql.functions import sum as spark_sum\n",
        "    total_rev = revenue_analysis.agg(spark_sum(\"total_revenue\").alias(\"total\")).collect()[0][\"total\"]\n",
        "    print(f\"Total revenue: ${total_rev:,.2f}\")\n",
        "\n",
        "print(\"\\n=== Analysis Complete ===\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Review exported CSV files in the output/ directory\")\n",
        "print(\"2. Import data into Power BI for visualization\")\n",
        "print(\"3. Connect to MongoDB collections for real-time dashboards\")\n",
        "print(\"4. Review project documentation in docs/ folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop Spark session\n",
        "spark.stop()\n",
        "print(\"Spark session stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
